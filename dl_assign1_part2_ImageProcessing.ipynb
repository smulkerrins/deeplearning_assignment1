{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecfbb4d-a2cb-4c76-95b4-158656ea2e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and set default options\n",
    "import datetime\n",
    "import calendar\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from array import array\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import NMF, PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import  datasets, layers, models\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, SimpleRNN, Input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e72bb1-9e84-4ba2-a543-d1f5428877fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_labels = ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351f825a-deca-47f3-8cfa-2da795b7c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnBlockOneAndTwoTrainSets():\n",
    "    (train_images, train_labels), (test_images, test_labels) = datasets.cifar100.load_data(label_mode='fine')\n",
    "\n",
    "    class_names = test_labels.reshape(-1) \n",
    "    # Normalise pixel values 0-1\n",
    "    train_images_normalised, test_images_normalised = train_images / 255.0, test_images / 255.0\n",
    "    \n",
    "    # Create a list of all classes\n",
    "    classes = np.arange(100)\n",
    "    \n",
    "    # Randomly shuffle the classes\n",
    "    np.random.shuffle(classes)\n",
    "    \n",
    "    block1_classes = classes[:50]\n",
    "    block2_classes = classes[50:]\n",
    "\n",
    "    train_labels = train_labels.reshape(-1)\n",
    "    test_labels = test_labels.reshape(-1)\n",
    "\n",
    "    train_mask1 = np.isin(train_labels, block1_classes)\n",
    "    train_mask2 = np.isin(train_labels, block2_classes)\n",
    "    test_mask1 = np.isin(test_labels, block1_classes)\n",
    "    test_mask2 = np.isin(test_labels, block2_classes)\n",
    "    \n",
    "    # Split the data into two subsets\n",
    "    # train_images_1, train_labels_1 = train_images_normalised[train_mask1], train_labels[train_mask1]\n",
    "    # train_images_2, train_labels_2 = train_images_normalised[train_mask2], train_labels[train_mask2]\n",
    "    # test_images_1, test_labels_1 = test_images_normalised[test_mask1], test_labels[test_mask1]\n",
    "    # test_images_2, test_labels_2 = test_images_normalised[test_mask2], test_labels[test_mask2]\n",
    "    train_images_1 = np.take(train_images_normalised, np.where(train_mask1)[0], axis=0)\n",
    "    train_labels_1 = np.take(train_labels, np.where(train_mask1)[0])\n",
    "    train_images_2 = np.take(train_images_normalised, np.where(train_mask2)[0], axis=0)\n",
    "    train_labels_2 = np.take(train_labels, np.where(train_mask2)[0])\n",
    "    test_images_1 = np.take(test_images_normalised, np.where(test_mask1)[0], axis=0)\n",
    "    test_labels_1 = np.take(test_labels, np.where(test_mask1)[0])\n",
    "    test_images_2 = np.take(test_images_normalised, np.where(test_mask2)[0], axis=0)\n",
    "    test_labels_2 = np.take(test_labels, np.where(test_mask2)[0])\n",
    "\n",
    "    return block1_classes, train_images_1, train_labels_1, test_images_1, test_labels_1, block2_classes, train_images_2, train_labels_2, test_images_2, test_labels_2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b881bfe2-e5c1-4a89-9476-dd7db463c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateAndAnalyse(model, history, test_images, test_labels, block_classes):\n",
    "    subset_class_names = np.unique([cifar100_labels[i] for i in block_classes])\n",
    "\n",
    "    # Generate predictions (using subset 1 for example)\n",
    "    predictions = model.predict(test_images)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    true_labels_text = np.array([cifar100_labels[label] for label in test_labels])\n",
    "    predicted_labels_text = np.array([cifar100_labels[label] for label in predicted_labels])\n",
    "    \n",
    "    # Confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(true_labels_text, predicted_labels_text, labels=subset_class_names) \n",
    "    \n",
    "    \n",
    "    # Plotting the confusion matrix\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Add labels to the plot\n",
    "    tick_marks = np.arange(len(subset_class_names))\n",
    "    plt.xticks(tick_marks, subset_class_names, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, subset_class_names)\n",
    "    \n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # from sklearn.metrics import classification_report\n",
    "    print(classification_report(test_labels, np.argmax(predictions, axis=1)))\n",
    "\n",
    "    report = classification_report(true_labels_text, predicted_labels_text, labels=subset_class_names, output_dict=True)\n",
    "\n",
    "    # Convert report to DataFrame\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    \n",
    "    # Sort by F1-score\n",
    "    df = df.sort_values(by='f1-score', ascending=False)\n",
    "    \n",
    "    # Print the DataFrame\n",
    "    print(df.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "\n",
    "    overall_accuracy = report['accuracy'] \n",
    "    print(f\"Overall accuracy: {overall_accuracy:.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bad5a55-5f3d-4bc4-b6ce-6ba8177b56bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTrainingAndValidationLoss(train_loss, val_loss ):\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    # modelConfig[\"plots\"].append(plt)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plotTrainingAndValidationAccuracy(train_acc, val_acc ):\n",
    "    plt.plot(train_acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    # modelConfig[\"plots\"].append(plt)\n",
    "    plt.show()\n",
    "    \n",
    "def plotModel(history):\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_acc = history.history['accuracy'] \n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    plotTrainingAndValidationLoss(train_loss, val_loss)\n",
    "    plotTrainingAndValidationAccuracy(train_acc, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67496477-95c8-40cf-8424-612b5339b96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compileAndTrainModel1(train_images, train_labels, test_images, test_labels):\n",
    "    # Standard basic model\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Convolutional layers\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    \n",
    "    # Flatten the output from convolutional layers\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # Hidden Dense layers\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    \n",
    "    # Output layer with 100 units for 100 classes\n",
    "    model.add(layers.Dense(100, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(train_images, train_labels, epochs=10, \n",
    "                        validation_data=(test_images, test_labels))\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae7f19-a38b-426f-ac08-98d00d7db57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compileAndTrainModel2(train_images, train_labels, test_images, test_labels):\n",
    "    # Standard model but with data augmentation through image generation\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Convolutional layers\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    \n",
    "    # Flatten the output from convolutional layers\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # Hidden Dense layers\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    \n",
    "    # Output layer with 100 units for 100 classes\n",
    "    model.add(layers.Dense(100, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    train_generator = datagen.flow(train_images, train_labels, batch_size=32) \n",
    "    history = model.fit(train_generator, epochs=10, validation_data=(test_images, test_labels))\n",
    "    \n",
    "    # Train the model\n",
    "    # history = model.fit(train_images, train_labels, epochs=10, \n",
    "    #                     validation_data=(test_images, test_labels))\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2af75dc-0c5c-4b34-ab94-6da26191bf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compileAndTrainModel3(train_images, train_labels, test_images, test_labels):\n",
    "    # Standard basic model 20 epochs\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Convolutional layers\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    \n",
    "    # Flatten the output from convolutional layers\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # Hidden Dense layers\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    \n",
    "    # Output layer with 100 units for 100 classes\n",
    "    model.add(layers.Dense(100, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(train_images, train_labels, epochs=20, \n",
    "                        validation_data=(test_images, test_labels))\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de7005b-90b2-465e-871b-78b8ccb32773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compileAndTrainModel4(train_images, train_labels, test_images, test_labels):\n",
    "    # Standard basic model ELU\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Convolutional layers\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    \n",
    "    # Flatten the output from convolutional layers\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # Hidden Dense layers\n",
    "    model.add(layers.Dense(128, activation='elu'))\n",
    "    model.add(layers.Dense(128, activation='elu'))\n",
    "    model.add(layers.Dense(128, activation='elu'))\n",
    "    model.add(layers.Dense(128, activation='elu'))\n",
    "    model.add(layers.Dense(128, activation='elu'))\n",
    "    \n",
    "    # Output layer with 100 units for 100 classes\n",
    "    model.add(layers.Dense(100, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(train_images, train_labels, epochs=10, \n",
    "                        validation_data=(test_images, test_labels))\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b2a513-b850-42b7-9f6a-e87dbe5cbb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compileAndTrainModel5(train_images, train_labels, test_images, test_labels):\n",
    "    # Standard basic model Leaky RELU\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Convolutional layers\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    \n",
    "    # Flatten the output from convolutional layers\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # Hidden Dense layers\n",
    "    model.add(layers.Dense(128))\n",
    "    model.add(layers.LeakyReLU(alpha=0.3))  # LeakyReLU with alpha=0.3\n",
    "    model.add(layers.Dense(128))\n",
    "    model.add(layers.LeakyReLU(alpha=0.3))  # LeakyReLU with alpha=0.3\n",
    "    model.add(layers.Dense(128))\n",
    "    model.add(layers.LeakyReLU(alpha=0.3))  # LeakyReLU with alpha=0.3\n",
    "    model.add(layers.Dense(128))\n",
    "    model.add(layers.LeakyReLU(alpha=0.3))  # LeakyReLU with alpha=0.3\n",
    "    model.add(layers.Dense(128))\n",
    "    model.add(layers.LeakyReLU(alpha=0.3))  # LeakyReLU with alpha=0.3\n",
    "    \n",
    "    # Output layer with 100 units for 100 classes\n",
    "    model.add(layers.Dense(100, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(train_images, train_labels, epochs=10, \n",
    "                        validation_data=(test_images, test_labels))\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2323873b-995b-446a-8403-1046c21d3a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compileAndTrainModel6(train_images, train_labels, test_images, test_labels):\n",
    "    # Standard basic model logistic / sigmoid\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    \n",
    "    # Output layer with 100 units for 100 classes\n",
    "    model.add(layers.Dense(100, activation='sigmoid'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    train_labels_encoded = tf.keras.utils.to_categorical(train_labels, num_classes=100)\n",
    "    test_labels_encoded = tf.keras.utils.to_categorical(test_images, num_classes=100)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(train_images, train_labels_encoded, epochs=10, \n",
    "                        validation_data=(test_images, test_labels_encoded))\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e121d62c-75a3-4959-a665-23021806d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compileAndTrainModel7(train_images, train_labels, test_images, test_labels):\n",
    "    # Standard basic model skip connections\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Define the input tensor\n",
    "    inputs = Input(shape=(32, 32, 3))\n",
    "    \n",
    "    # Convolutional layers with skip connections\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu')(inputs)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x_skip = x  # Store the output for the skip connection\n",
    "    \n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu')(x)\n",
    "\n",
    "    x_skip = layers.Conv2D(128, (1, 1), strides=4, activation='relu')(x_skip)  # Adjust channels and downsample\n",
    "    x = layers.add([x, x_skip]) \n",
    "    \n",
    "    # Flatten and dense layers with ReLU\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    outputs = layers.Dense(100, activation='softmax')(x)\n",
    "    \n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(train_images, train_labels, epochs=10, \n",
    "                        validation_data=(test_images, test_labels))\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1f41bd-86f8-48dc-a308-67a10afeb239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compileAndTrainModel8(train_images, train_labels, test_images, test_labels):\n",
    "    # Standard basic model Leaky RELU, 30 epochs, early stopping\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Convolutional layers\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    \n",
    "    # Flatten the output from convolutional layers\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # Hidden Dense layers\n",
    "    model.add(layers.Dense(128))\n",
    "    model.add(layers.LeakyReLU(alpha=0.3)) \n",
    "    model.add(layers.Dense(128))\n",
    "    model.add(layers.LeakyReLU(alpha=0.3)) \n",
    "    model.add(layers.Dense(128))\n",
    "    model.add(layers.LeakyReLU(alpha=0.3)) \n",
    "    model.add(layers.Dense(128))\n",
    "    model.add(layers.LeakyReLU(alpha=0.3)) \n",
    "    model.add(layers.Dense(128))\n",
    "    model.add(layers.LeakyReLU(alpha=0.3)) \n",
    "    \n",
    "    # Output layer with 100 units for 100 classes\n",
    "    model.add(layers.Dense(100, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(train_images, train_labels, epochs=30, \n",
    "                        validation_data=(test_images, test_labels),\n",
    "                        callbacks=[early_stopping])\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271f6871-b25e-4e07-8b9f-b62f4dd59fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78500e2-1ceb-4f8d-9d3c-4645681d78cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ade5b30-7699-44df-8257-447cd303de2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "block1_classes, train_images_1, train_labels_1, test_images_1, test_labels_1, block2_classes, train_images_2, train_labels_2, test_images_2, test_labels_2 = returnBlockOneAndTwoTrainSets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcbc252-cf19-4b1b-bc7c-abbbba1086b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, history = compileAndTrainModel1(train_images_1, train_labels_1, test_images_1, test_labels_1)\n",
    "evaluateAndAnalyse(model, history, test_images_1, test_labels_1, block1_classes)\n",
    "plotModel(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb341b4-8c6c-4f24-9b87-506a401a3d26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2, history2 = compileAndTrainModel2(train_images_1, train_labels_1, test_images_1, test_labels_1)\n",
    "evaluateAndAnalyse(model2, history2, test_images_1, test_labels_1, block1_classes)\n",
    "plotModel(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06098b4e-7bcc-4f20-81cc-c164968506f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model3, history3 = compileAndTrainModel3(train_images_1, train_labels_1, test_images_1, test_labels_1)\n",
    "evaluateAndAnalyse(model3, history3, test_images_1, test_labels_1, block1_classes)\n",
    "plotModel(history3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4845bd5a-ee9f-4588-b192-0ffb9aa65c68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model4, history4 = compileAndTrainModel4(train_images_1, train_labels_1, test_images_1, test_labels_1)\n",
    "evaluateAndAnalyse(model4, history4, test_images_1, test_labels_1, block1_classes)\n",
    "plotModel(history4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce278c9e-89d3-47d1-b4b0-aa904324d83c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model5, history5 = compileAndTrainModel(train_images_1, train_labels_1, test_images_1, test_labels_1)\n",
    "evaluateAndAnalyse(model5, history5, test_images_1, test_labels_1, block1_classes)\n",
    "plotModel(history5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d183e464-351a-425a-a68c-8e4a07549304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fails due to dimension mismmatches\n",
    "# model6, history6 = compileAndTrainModel6(train_images_1, train_labels_1, test_images_1, test_labels_1)\n",
    "# evaluateAndAnalyse(model6, history6, test_images_1, test_labels_1, block1_classes)\n",
    "# plotModel(history6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c3937f-3946-4abc-a56b-5c8185bd3e3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model7, history7 = compileAndTrainModel7(train_images_1, train_labels_1, test_images_1, test_labels_1)\n",
    "evaluateAndAnalyse(model7, history7, test_images_1, test_labels_1, block1_classes)\n",
    "plotModel(history7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e1e157-e813-4fee-b954-b78027fd8d54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model8, history8 = compileAndTrainModel8(train_images_1, train_labels_1, test_images_1, test_labels_1)\n",
    "evaluateAndAnalyse(model8, history8, test_images_1, test_labels_1, block1_classes)\n",
    "plotModel(history8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1833ea9d-1f37-4482-a8a8-f6e946839536",
   "metadata": {},
   "outputs": [],
   "source": [
    "kerasFileName = 'part2saves/part2imgclass_bestmodel.h5'\n",
    "model8.save(kerasFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c71a2a6-8d43-4930-8cf8-0f890feeed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "kerasFileName = 'part2saves/part2imgclass_besthistory.h5'\n",
    "history8.save(kerasFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6746a7-09b4-46eb-835b-d64cf72a01c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelFileName = 'part2saves/part2imgclass_bestmodel.pkl'\n",
    "# modelFile = open(modelFileName, 'ab')\n",
    "# pickle.dump(model8, modelFile)                    \n",
    "# modelFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515658a0-6c8a-46a2-a2c0-8d1e961538a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelFileName = 'part2saves/part2imgclass_besthistory.pkl'\n",
    "modelFile = open(modelFileName, 'ab')\n",
    "pickle.dump(history8.history, modelFile)                    \n",
    "modelFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fb448f-bb2d-4658-9630-9d33eea07223",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(history8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc593a-f77d-4552-bf3a-fa77af3b8dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model8\n",
    "best_history = history8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab39fac4-8f7a-4148-a3e4-6224e1b11854",
   "metadata": {},
   "source": [
    "\n",
    "## Autoencoder Modelling: For Block 1 Images \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef93f84-e500-47d5-b003-8b9ca55b4ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateAutoencoder(autoencoder, test_images):\n",
    "    mse = autoencoder.evaluate(test_images, test_images)  # Calculate MSE on the test set\n",
    "    print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84283970-2ff6-4d17-bc0a-7158c576b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualTestAutoencoder(autoencoder, test_images):\n",
    "    reconstructed_images = autoencoder.predict(test_images)\n",
    "\n",
    "    for i in range(5):\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(test_images[i])\n",
    "        plt.title(\"Original\")\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(reconstructed_images[i])\n",
    "        plt.title(\"Reconstructed\")\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218e3b64-0f93-498d-8bf8-5b3f94711e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotAutoencoder(history):\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    # train_acc = history.history['accuracy'] \n",
    "    # val_acc = history.history['val_accuracy']\n",
    "\n",
    "    plotTrainingAndValidationLoss(train_loss, val_loss)\n",
    "    # plotTrainingAndValidationAccuracy(train_acc, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedaf027-f8d5-4f46-87fd-86930c9bcfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compileAndTrainAutoencoder1(original_model,train_images, train_labels, test_images, test_labels):\n",
    "    encoder = models.Sequential()\n",
    "    for layer in original_model.layers[:5]:  # Take the first 5 layers (convolutions and flatten)\n",
    "        encoder.add(layer)\n",
    "    \n",
    "    # --- Decoder ---\n",
    "    decoder = models.Sequential()\n",
    "    decoder.add(layers.Conv2DTranspose(128, (3, 3), activation='relu', padding='same', input_shape=(4, 4, 128)))\n",
    "    decoder.add(layers.UpSampling2D((2, 2)))\n",
    "    decoder.add(layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same'))\n",
    "    decoder.add(layers.UpSampling2D((2, 2)))\n",
    "    decoder.add(layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same'))\n",
    "    decoder.add(layers.UpSampling2D((2, 2)))\n",
    "    decoder.add(layers.Conv2DTranspose(3, (3, 3), activation='sigmoid', padding='same'))  # Output layer\n",
    "\n",
    "    # --- Autoencoder ---\n",
    "    autoencoder = models.Sequential([encoder, decoder])\n",
    "    \n",
    "    # Compile the model\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # Train the autoencoder\n",
    "    history = autoencoder.fit(train_images, train_images, epochs=10, batch_size=32, validation_split=0.1)\n",
    "    \n",
    "    return autoencoder, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09ec45b-149c-415f-b22a-ee3056e11acc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder, autoencoder_history = compileAndTrainAutoencoder1(best_model,train_images_1, train_labels_1, test_images_1, test_labels_1)\n",
    "plotAutoencoder(autoencoder_history)\n",
    "visualTestAutoencoder(autoencoder, test_images_1)\n",
    "evaluateAutoencoder(autoencoder, test_images_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83bd59d-b7ae-4055-be8d-40b1ed755ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kerasFileName = 'part2saves/part2imgclass_autoencoder.h5'\n",
    "autoencoder.save(kerasFileName)\n",
    "\n",
    "modelFileName = 'part2saves/part2imgclass_autoencoderhistory.pkl'\n",
    "modelFile = open(modelFileName, 'ab')\n",
    "pickle.dump(autoencoder_history, modelFile)                    \n",
    "modelFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d298f150-d061-48e1-9b87-245e1d86b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelFileName = 'part2saves/part2imgclass_autoencoder.pkl'\n",
    "# modelFile = open(modelFileName, 'ab')\n",
    "# pickle.dump(autoencoder, modelFile)                    \n",
    "# modelFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7dc7a-5509-4806-afd1-71ec45006af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5cc47b5-4be7-4c80-b05c-fda039407468",
   "metadata": {},
   "source": [
    "## Transfer Learning for Block 2 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe8007-e90a-443b-8d4d-47fc8f938c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compileAndTrainTransferModel(model, train_images, train_labels, test_images, test_labels):\n",
    "    # Freeze the convolutional layers\n",
    "    for layer in model.layers[:5]:  # The first 5 layers are convolutional\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Create a new model for transfer learning\n",
    "    transfer_model = models.Sequential()\n",
    "    transfer_model.add(model)  # Add the original model as the base\n",
    "    \n",
    "    # Add new classification layers\n",
    "    transfer_model.add(layers.Dense(128))\n",
    "    transfer_model.add(layers.LeakyReLU(alpha=0.3)) \n",
    "    transfer_model.add(layers.Dense(128))\n",
    "    transfer_model.add(layers.LeakyReLU(alpha=0.3)) \n",
    "    transfer_model.add(layers.Dense(128))\n",
    "    transfer_model.add(layers.LeakyReLU(alpha=0.3)) \n",
    "    # transfer_model.add(layers.Dense(128))\n",
    "    # transfer_model.add(layers.LeakyReLU(alpha=0.3)) \n",
    "    # transfer_model.add(layers.Dense(128))\n",
    "    # transfer_model.add(layers.LeakyReLU(alpha=0.3)) \n",
    "\n",
    "    \n",
    "    transfer_model.add(layers.Dense(50, activation='softmax'))  # 50 output classes\n",
    "    \n",
    "    # Compile the transfer learning model\n",
    "    transfer_model.compile(optimizer='adam',\n",
    "                          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                          metrics=['accuracy'])\n",
    "    \n",
    "    # Train the transfer learning model on the second subset\n",
    "    history = transfer_model.fit(train_images, train_labels, epochs=30, \n",
    "                                validation_data=(test_images, test_labels),\n",
    "                                callbacks=[early_stopping])\n",
    "\n",
    "    return transfer_model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a1e893-03bf-457a-a709-d0dcda155353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fineTuneTransferModel(model, transfer_model, train_images, train_labels, test_images, test_labels):\n",
    "    for layer in model.layers[3:]:  \n",
    "        layer.trainable = True\n",
    "    \n",
    "    transfer_model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),  \n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    history = transfer_model.fit(train_images, train_labels, epochs=10, \n",
    "                                validation_data=(test_images, test_labels))\n",
    "\n",
    "    return transfer_model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d852f8-a218-4118-9188-589b4eb15029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2944d7-9d32-40df-9b90-1f10c523254b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelBlock2FromScratch, historyBlock2FromScratch = compileAndTrainModel8(train_images_2, train_labels_2, test_images_2, test_labels_2)\n",
    "evaluateAndAnalyse(modelBlock2FromScratch, historyBlock2FromScratch, test_images_2, test_labels_2, block1_classes)\n",
    "plotModel(historyBlock2FromScratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4198b550-d9b3-4d8e-ba7e-fbf012fcaf2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelBlock2FromTransfer, historyBlock2FromTransfer = compileAndTrainTransferModel(best_model, train_images_2, train_labels_2, test_images_2, test_labels_2)\n",
    "evaluateAndAnalyse(modelBlock2FromTransfer, historyBlock2FromTransfer, test_images_2, test_labels_2, block2_classes)\n",
    "plotModel(historyBlock2FromTransfer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a4cc40-6936-4718-8752-ccb44a589062",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelBlock2FromFineTune, historyBlock2FromFineTune = fineTuneTransferModel(best_model, modelBlock2FromTransfer, train_images_2, train_labels_2, test_images_2, test_labels_2)\n",
    "evaluateAndAnalyse(modelBlock2FromFineTune, historyBlock2FromFineTune, test_images_2, test_labels_2, block2_classes)\n",
    "plotModel(historyBlock2FromFineTune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fd34dc-fd2d-4ca3-a26e-2f7d707d3ab0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_images_2\n",
    "modelFileName = 'part2saves/part2_block2_classes.pkl'\n",
    "modelFile = open(modelFileName, 'ab')\n",
    "pickle.dump(block2_classes, modelFile)                    \n",
    "modelFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4068dac-fb03-4f0a-bfe0-65b599d5dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "block2_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c83272-a558-4d37-8943-8d3187bb84c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ad6de1-3c85-49b3-b375-482f6ad98456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545ea78c-ebc7-404d-b33d-aa2092c5b6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
