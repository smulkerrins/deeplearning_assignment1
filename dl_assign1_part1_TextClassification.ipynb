{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c31585-b936-4b8e-8c8c-4351f7e8f383",
   "metadata": {},
   "source": [
    "##Â Notebook config\n",
    "\n",
    "mode = 'train' will load the data from train.csv and test.csv, train multiple models with a variety of hyperparamters, evaluate their performance and save the top two performing models\n",
    "\n",
    "mode = 'demo' will load the previously saved top two performing models and will demonstrate their performance against data from test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0177e49a-9ef6-4532-9f96-236ed592a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'train'\n",
    "# mode = 'demo'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ae2e8e-0aa6-4488-afd1-b19254271c35",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e970069-d4ba-41bf-97c3-e97c5c6e50e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and set default options\n",
    "import datetime\n",
    "import calendar\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from array import array\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import NMF, PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, SimpleRNN, Input, concatenate, Flatten\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pmdarima as pm\n",
    "from pmdarima.model_selection import train_test_split as pmd_train_test_split\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64f5148-daa5-429b-8f7b-44e612b77d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTrainingData(limitSize = True, limit = 1000):\n",
    "    df_train = pd.read_csv('train.csv')\n",
    "    df_train = df_train.dropna()\n",
    "\n",
    "    if(limitSize):\n",
    "        return df_train[:limit]\n",
    "    else:\n",
    "        return df_train\n",
    "\n",
    "def loadTestData():\n",
    "    df_test = pd.read_csv('test.csv')\n",
    "    df_test = df_test.dropna()\n",
    "\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279aab62-387a-4138-ad5e-6924c396af91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitArtistLyricsAndGenres(df_train, df_test):\n",
    "    lyrics_train = df_train['Lyrics'].values\n",
    "    genres_train = df_train['Genre'].values\n",
    "    artist_train = df_train['Artist'].values\n",
    "\n",
    "    lyrics_test = df_test['Lyrics'].values\n",
    "    genres_test = df_test['Genre'].values\n",
    "    artist_test = df_test['Artist'].values\n",
    "\n",
    "    return artist_train, lyrics_train, genres_train, artist_test, lyrics_test, genres_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faf2d8b-804b-4af5-b810-19ccf9a79a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateEncodedPaddedSequences(artist_train, lyrics_train, genres_train, lyrics_test, artist_test, genres_test, max_length):\n",
    "    # ngram_range = 3\n",
    "    artist_tokenizer = Tokenizer(num_words=1000, oov_token='<OOV>') # Adjust num_words as needed\n",
    "    artist_tokenizer.fit_on_texts(np.union1d(artist_train, artist_test))\n",
    "    encoded_artist_train = artist_tokenizer.texts_to_sequences(artist_train)\n",
    "    encoded_artist_test = artist_tokenizer.texts_to_sequences(artist_test)\n",
    "    \n",
    "    lyric_tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
    "    lyric_tokenizer.fit_on_texts(np.union1d(lyrics_train, lyrics_test))\n",
    "    \n",
    "    lyric_sequences_train = lyric_tokenizer.texts_to_sequences(lyrics_train)\n",
    "    lyric_sequences_test = lyric_tokenizer.texts_to_sequences(lyrics_test)\n",
    "    \n",
    "    genre_tokenizer = Tokenizer(filters='!\"#$%()*+,./:;<=>?@[\\\\]^_`{|}~\\t\\n') #filters out symbols from genres e.g. R&B, Hip-Hop\n",
    "    genre_tokenizer.fit_on_texts(np.union1d(genres_train, genres_test))\n",
    "    genre_index = genre_tokenizer.word_index\n",
    "    encoded_genres_train = genre_tokenizer.texts_to_sequences(genres_train)\n",
    "    encoded_genres_test = genre_tokenizer.texts_to_sequences(genres_test)\n",
    "    \n",
    "    # max_length = 500 # Adjust as needed\n",
    "    padded_sequences_train = pad_sequences(lyric_sequences_train, maxlen=max_length, truncating='post')\n",
    "    padded_sequences_test = pad_sequences(lyric_sequences_test, maxlen=max_length, truncating='post')\n",
    "\n",
    "    encoded_artist_train = pad_sequences(encoded_artist_train, maxlen=10, truncating='post')\n",
    "    encoded_artist_test = pad_sequences(encoded_artist_test, maxlen=10, truncating='post')\n",
    "    encoded_artist_array_train = np.array(encoded_artist_train)\n",
    "    encoded_artist_array_test = np.array(encoded_artist_test)\n",
    "    \n",
    "    padded_sequences_array_train = np.array(padded_sequences_train)\n",
    "    padded_sequences_array_test = np.array(padded_sequences_test)\n",
    "    \n",
    "    encoded_genres_array_train = np.array(encoded_genres_train)\n",
    "    encoded_genres_array_test = np.array(encoded_genres_test)\n",
    "    \n",
    "    return encoded_artist_array_train, encoded_artist_array_test, padded_sequences_array_train, padded_sequences_array_test, encoded_genres_array_train, encoded_genres_array_test, genre_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25814a13-b576-4dbb-94a7-32d1ced02d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compileModel(embeddingInputDim, embeddingOutputDim, layers, activationFunction, lossFunction, optimizerFunction, max_length, genre_index, include_artist):\n",
    "    # model = None # probably don't need this anymore\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    if(not include_artist):\n",
    "        model.add(Embedding(embeddingInputDim, embeddingOutputDim, input_length=max_length))\n",
    "        for layer in layers:\n",
    "            model.add(layer)\n",
    "        model.add(Dense(len(genre_index) + 1, activation=activationFunction)) \n",
    "    else:\n",
    "        lyrics_input = Input(shape=(max_length,))\n",
    "        lyrics_embedding = Embedding(embeddingInputDim, embeddingOutputDim, input_length=max_length)(lyrics_input)\n",
    "        lyrics_lstm = LSTM(64)(lyrics_embedding)\n",
    "        artist_input = Input(shape=(10,))\n",
    "        artist_embedding = Embedding(1000, 32)(artist_input)\n",
    "        artist_flatten = Flatten()(artist_embedding)\n",
    "        concatenated = concatenate([lyrics_lstm, artist_flatten])\n",
    "        output = Dense(len(genre_index) + 1, activation=activationFunction)(concatenated)\n",
    "        model = Model(inputs=[lyrics_input, artist_input], outputs=output)\n",
    "    \n",
    "    \n",
    "    model.compile(loss=lossFunction, optimizer=optimizerFunction, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9049c72a-ddc1-4c75-a5f2-85c236d3ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, batch_size, encoded_artist_array_train, padded_sequences_array_train, encoded_genres_array_train, epochs, include_artist):\n",
    "    batch_size = 32\n",
    "    \n",
    "    if(not include_artist):\n",
    "        history = model.fit(padded_sequences_array_train, encoded_genres_array_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "    else:\n",
    "        history = model.fit([padded_sequences_array_train,encoded_artist_array_train], encoded_genres_array_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4af8f8-65ea-4025-b474-2352762e30b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTrainingAndValidationLoss(train_loss, val_loss, modelConfig ):\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    # modelConfig[\"plots\"].append(plt)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plotTrainingAndValidationAccuracy(train_acc, val_acc, modelConfig ):\n",
    "    plt.plot(train_acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    # modelConfig[\"plots\"].append(plt)\n",
    "    plt.show()\n",
    "\n",
    "def plotModel(history, modelConfig):\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_acc = history.history['accuracy'] \n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    # modelConfig[\"plots\"].append(plotTrainingAndValidationLoss(train_loss, val_loss, modelConfig))\n",
    "    # modelConfig[\"plots\"].append(plotTrainingAndValidationAccuracy(train_acc, val_acc, modelConfig))\n",
    "    plotTrainingAndValidationLoss(train_loss, val_loss, modelConfig)\n",
    "    plotTrainingAndValidationAccuracy(train_acc, val_acc, modelConfig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2a3cf6-0f98-4eaa-ab57-9fefe70e2666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(model, encoded_artist_array_test, padded_sequences_array_test, encoded_genres_array_test, include_artist):\n",
    "    \n",
    "    if(not include_artist):\n",
    "        test_loss, test_acc = model.evaluate(padded_sequences_array_test, encoded_genres_array_test)\n",
    "    else:\n",
    "        test_loss, test_acc = model.evaluate([padded_sequences_array_test, encoded_artist_array_test], encoded_genres_array_test)\n",
    "\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83848c38-8645-404e-a342-93c3e1609a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusionMatrix(modelConfig, encoded_artist_array_test, padded_sequences_array_test, encoded_genres_array_test, genre_labels, include_artist):\n",
    "    model = modelConfig['model']\n",
    "    if(not include_artist):\n",
    "        predictions = model.predict(padded_sequences_array_test)\n",
    "    else:\n",
    "        predictions = model.predict([padded_sequences_array_test,encoded_artist_array_test])\n",
    "    predicted_labels = tf.argmax(predictions, axis=1)\n",
    "    cm = confusion_matrix(encoded_genres_array_test, predicted_labels) \n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sb.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=genre_labels, yticklabels=genre_labels)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    # modelConfig['cm_plots'].append(plt)\n",
    "    plt.show()\n",
    "\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5a7173-b5eb-4488-a047-6db00a0a3d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModelConfigs():\n",
    "    modelConfigs = []\n",
    "\n",
    "    modelConfig1 = {\n",
    "        \"name\": \"1 Single LSTM 64 Layer - scc - softmax - adam\",\n",
    "        \"includeArtist\": False,\n",
    "        \"embeddingInputDim\": 5000,\n",
    "        \"embeddingOutputDim\": 128,\n",
    "        \"max_length\": 500,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 10,\n",
    "        \"layers\": [\n",
    "            LSTM(64)\n",
    "        ],\n",
    "        \"lossFunction\": 'sparse_categorical_crossentropy',\n",
    "        \"activationFunction\": 'softmax',\n",
    "        \"optimizerFunction\": 'adam',\n",
    "        \"plots\": [], \"cm_plots\": [], \"model\": None, \"history\": None, \"test_loss\" : None, \"test_acc\" : None\n",
    "    }\n",
    "    modelConfig2 = {\n",
    "        \"name\": \"2 Single LSTM 128 Layer - scc - softmax - adam\",\n",
    "        \"includeArtist\": False,\n",
    "        \"embeddingInputDim\": 5000,\n",
    "        \"embeddingOutputDim\": 128,\n",
    "        \"max_length\": 500,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 10,\n",
    "        \"layers\": [\n",
    "            LSTM(128)\n",
    "        ],\n",
    "        \"lossFunction\": 'sparse_categorical_crossentropy',\n",
    "        \"activationFunction\": 'softmax',\n",
    "        \"optimizerFunction\": 'adam',\n",
    "        \"plots\": [], \"cm_plots\": [], \"model\": None, \"history\": None, \"test_loss\" : None, \"test_acc\" : None\n",
    "    }\n",
    "    modelConfig3 = {\n",
    "        \"name\": \"3 Double LSTM 64 Layer dropout - scc - softmax - adam\",\n",
    "        \"includeArtist\": False,\n",
    "        \"embeddingInputDim\": 5000,\n",
    "        \"embeddingOutputDim\": 128,\n",
    "        \"max_length\": 500,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 10,\n",
    "        \"layers\": [\n",
    "            LSTM(units=64, dropout=0.2, return_sequences=True),\n",
    "            LSTM(64, dropout=0.2)\n",
    "        ],\n",
    "        \"lossFunction\": 'sparse_categorical_crossentropy',\n",
    "        \"activationFunction\": 'softmax',\n",
    "        \"optimizerFunction\": 'adam',\n",
    "        \"plots\": [], \"cm_plots\": [], \"model\": None, \"history\": None, \"test_loss\" : None, \"test_acc\" : None\n",
    "    }\n",
    "    modelConfig4 = {\n",
    "        \"name\": \"4 Double LSTM 128 Layer Dropout - scc - softmax - adam\",\n",
    "        \"includeArtist\": False,\n",
    "        \"embeddingInputDim\": 5000,\n",
    "        \"embeddingOutputDim\": 128,\n",
    "        \"max_length\": 500,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 10,\n",
    "        \"layers\": [\n",
    "            LSTM(units=128, dropout=0.2, return_sequences=True),\n",
    "            LSTM(128, dropout=0.2)\n",
    "        ],\n",
    "        \"lossFunction\": 'sparse_categorical_crossentropy',\n",
    "        \"activationFunction\": 'softmax',\n",
    "        \"optimizerFunction\": 'adam',\n",
    "        \"plots\": [], \"cm_plots\": [], \"model\": None, \"history\": None, \"test_loss\" : None, \"test_acc\" : None\n",
    "    }\n",
    "    modelConfig5 = {\n",
    "        \"name\": \"5 Single LSTM 128 Layer - cc - softmax - adam\",\n",
    "        \"includeArtist\": False,\n",
    "        \"embeddingInputDim\": 5000,\n",
    "        \"embeddingOutputDim\": 128,\n",
    "        \"max_length\": 500,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 10,\n",
    "        \"layers\": [\n",
    "            LSTM(128)\n",
    "        ],\n",
    "        \"lossFunction\": 'categorical_crossentropy',\n",
    "        \"activationFunction\": 'softmax',\n",
    "        \"optimizerFunction\": 'adam',\n",
    "        \"plots\": [], \"cm_plots\": [], \"model\": None, \"history\": None, \"test_loss\" : None, \"test_acc\" : None\n",
    "    }\n",
    "    modelConfig6 = {\n",
    "        \"name\": \"6 Single LSTM 128 Layer - scc - softmax - SGD\",\n",
    "        \"includeArtist\": False,\n",
    "        \"embeddingInputDim\": 5000,\n",
    "        \"embeddingOutputDim\": 128,\n",
    "        \"max_length\": 500,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 10,\n",
    "        \"layers\": [\n",
    "            LSTM(128)\n",
    "        ],\n",
    "        \"lossFunction\": 'sparse_categorical_crossentropy',\n",
    "        \"activationFunction\": 'softmax',\n",
    "        \"optimizerFunction\": 'sgd',\n",
    "        \"plots\": [], \"cm_plots\": [], \"model\": None, \"history\": None, \"test_loss\" : None, \"test_acc\" : None\n",
    "    }\n",
    "\n",
    "    modelConfig7 = {\n",
    "        \"name\": \"7 Single LSTM 64 Layer - scc - relu - adam\",\n",
    "        \"includeArtist\": False,\n",
    "        \"embeddingInputDim\": 5000,\n",
    "        \"embeddingOutputDim\": 128,\n",
    "        \"max_length\": 500,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 10,\n",
    "        \"layers\": [\n",
    "            LSTM(64)\n",
    "        ],\n",
    "        \"lossFunction\": 'sparse_categorical_crossentropy',\n",
    "        \"activationFunction\": 'relu',\n",
    "        \"optimizerFunction\": 'adam',\n",
    "        \"plots\": [], \"cm_plots\": [], \"model\": None, \"history\": None, \"test_loss\" : None, \"test_acc\" : None\n",
    "    }\n",
    "    modelConfig8 = {\n",
    "        \"name\": \"8 Triple LSTM 128 Layer Dropout - scc - softmax - adam\",\n",
    "        \"includeArtist\": False,\n",
    "        \"embeddingInputDim\": 5000,\n",
    "        \"embeddingOutputDim\": 128,\n",
    "        \"max_length\": 500,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 10,\n",
    "        \"layers\": [\n",
    "            LSTM(units=128, dropout=0.2, return_sequences=True),\n",
    "            LSTM(units=128, dropout=0.2, return_sequences=True),\n",
    "            LSTM(128, dropout=0.2)\n",
    "        ],\n",
    "        \"lossFunction\": 'sparse_categorical_crossentropy',\n",
    "        \"activationFunction\": 'softmax',\n",
    "        \"optimizerFunction\": 'adam',\n",
    "        \"plots\": [], \"cm_plots\": [], \"model\": None, \"history\": None, \"test_loss\" : None, \"test_acc\" : None\n",
    "    }\n",
    "    modelConfig9 = {\n",
    "        \"name\": \"9 Single RNN 64 Layer - scc - softmax - adam\",\n",
    "        \"includeArtist\": False,\n",
    "        \"embeddingInputDim\": 1000,\n",
    "        \"embeddingOutputDim\": 64,\n",
    "        \"max_length\": 500,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 10,\n",
    "        \"layers\": [\n",
    "            SimpleRNN(64)\n",
    "        ],\n",
    "        \"lossFunction\": 'sparse_categorical_crossentropy',\n",
    "        \"activationFunction\": 'softmax',\n",
    "        \"optimizerFunction\": 'adam',\n",
    "        \"plots\": [], \"cm_plots\": [], \"model\": None, \"history\": None, \"test_loss\" : None, \"test_acc\" : None\n",
    "    }\n",
    "    modelConfig10 = {\n",
    "        \"name\": \"10 Simple RNN 64 before LSTM 64 Layer - scc - softmax - adam\",\n",
    "        \"includeArtist\": False,\n",
    "        \"embeddingInputDim\": 1000,\n",
    "        \"embeddingOutputDim\": 64,\n",
    "        \"max_length\": 500,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 10,\n",
    "        \"layers\": [\n",
    "            SimpleRNN(64, return_sequences=True),\n",
    "            LSTM(64)\n",
    "        ],\n",
    "        \"lossFunction\": 'sparse_categorical_crossentropy',\n",
    "        \"activationFunction\": 'softmax',\n",
    "        \"optimizerFunction\": 'adam',\n",
    "        \"plots\": [], \"cm_plots\": [], \"model\": None, \"history\": None, \"test_loss\" : None, \"test_acc\" : None\n",
    "    }\n",
    "    modelConfig11 = {\n",
    "        \"name\": \"11 Single LSTM 64 Layer 20 epochs - scc - softmax - adam\",\n",
    "        \"includeArtist\": False,\n",
    "        \"embeddingInputDim\": 5000,\n",
    "        \"embeddingOutputDim\": 128,\n",
    "        \"max_length\": 500,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 20,\n",
    "        \"layers\": [\n",
    "            LSTM(64)\n",
    "        ],\n",
    "        \"lossFunction\": 'sparse_categorical_crossentropy',\n",
    "        \"activationFunction\": 'softmax',\n",
    "        \"optimizerFunction\": 'adam',\n",
    "        \"plots\": [], \"cm_plots\": [], \"model\": None, \"history\": None, \"test_loss\" : None, \"test_acc\" : None\n",
    "    }\n",
    "    modelConfig12 = {\n",
    "        \"name\": \"12 Single LSTM 64 Layer Include Artist - scc - softmax - adam\",\n",
    "        \"includeArtist\": True,\n",
    "        \"embeddingInputDim\": 5000,\n",
    "        \"embeddingOutputDim\": 128,\n",
    "        \"max_length\": 500,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 10,\n",
    "        \"layers\": [\n",
    "            LSTM(64)\n",
    "        ],\n",
    "        \"lossFunction\": 'sparse_categorical_crossentropy',\n",
    "        \"activationFunction\": 'softmax',\n",
    "        \"optimizerFunction\": 'adam',\n",
    "        \"plots\": [], \"cm_plots\": [], \"model\": None, \"history\": None, \"test_loss\" : None, \"test_acc\" : None\n",
    "    }\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    modelConfigs.append(modelConfig1)\n",
    "    modelConfigs.append(modelConfig2)\n",
    "    modelConfigs.append(modelConfig3)\n",
    "    modelConfigs.append(modelConfig4)\n",
    "    # # modelConfigs.append(modelConfig5)\n",
    "    # # modelConfigs.append(modelConfig6)\n",
    "    # modelConfigs.append(modelConfig7)\n",
    "    # # modelConfigs.append(modelConfig8)\n",
    "    # # modelConfigs.append(modelConfig9)\n",
    "    # # modelConfigs.append(modelConfig10)\n",
    "    modelConfigs.append(modelConfig11)\n",
    "    modelConfigs.append(modelConfig12)\n",
    "\n",
    "    return modelConfigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e276d117-e850-44a4-af81-ca86013d1ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_train = loadTrainingData(limitSize = True, limit = 1000)\n",
    "df_train = loadTrainingData(limitSize = False, limit = 0)\n",
    "df_test = loadTestData()\n",
    "\n",
    "print(f\"Training for: {len(df_train)} rows\")\n",
    "print(f\"Validating for: {len(df_test)} rows\")\n",
    "\n",
    "max_length = 500\n",
    "artist_train, lyrics_train, genres_train, artist_test, lyrics_test, genres_test = splitArtistLyricsAndGenres(df_train, df_test)\n",
    "encoded_artist_array_train, encoded_artist_array_test, padded_sequences_array_train, padded_sequences_array_test, encoded_genres_array_train, encoded_genres_array_test, genre_index = generateEncodedPaddedSequences(artist_train, lyrics_train, genres_train, artist_test, lyrics_test, genres_test, max_length)\n",
    "genre_labels = list(genre_index.keys())\n",
    "print(f\"Encoding and Sequencing completed\\n\\n\")\n",
    "\n",
    "modelConfigs = loadModelConfigs()\n",
    "for modelConfig in modelConfigs:\n",
    "    print(f\"Model: {modelConfig['name']}\")\n",
    "    print(f\"Compiling: {modelConfig['name']}\")\n",
    "    modelConfig['model'] = compileModel(\n",
    "        embeddingInputDim=modelConfig['embeddingInputDim'],\n",
    "        embeddingOutputDim=modelConfig['embeddingOutputDim'],\n",
    "        layers=modelConfig['layers'],\n",
    "        activationFunction=modelConfig['activationFunction'],\n",
    "        lossFunction=modelConfig['lossFunction'],\n",
    "        optimizerFunction=modelConfig['optimizerFunction'],\n",
    "        max_length=max_length,\n",
    "        genre_index=genre_index,\n",
    "        include_artist = modelConfig['includeArtist']\n",
    "    )\n",
    "\n",
    "    print(f\"Training: {modelConfig['name']}\")\n",
    "    modelConfig['history'] = trainModel(\n",
    "        model=modelConfig['model'],\n",
    "        batch_size=modelConfig['model'],\n",
    "        encoded_artist_array_train = encoded_artist_array_train, \n",
    "        padded_sequences_array_train=padded_sequences_array_train,\n",
    "        encoded_genres_array_train=encoded_genres_array_train,\n",
    "        epochs=modelConfig['epochs'],\n",
    "        include_artist = modelConfig['includeArtist']\n",
    "    )\n",
    "\n",
    "    print(f\"Plotting: {modelConfig['name']}\")\n",
    "    plotModel(modelConfig['history'], modelConfig)\n",
    "\n",
    "    print(f\"Confusion Matrix: {modelConfig['name']}\")\n",
    "    cm = confusionMatrix(modelConfig, encoded_artist_array_test, padded_sequences_array_test, encoded_genres_array_test, genre_labels, modelConfig['includeArtist'])\n",
    "    print(cm)\n",
    "\n",
    "    print(f\"Evaluating: {modelConfig['name']}\")\n",
    "    test_loss, test_acc = evaluateModel(\n",
    "        model=modelConfig['model'],\n",
    "        encoded_artist_array_test = encoded_artist_array_test, \n",
    "        padded_sequences_array_test=padded_sequences_array_test,\n",
    "        encoded_genres_array_test=encoded_genres_array_test,\n",
    "        include_artist = modelConfig['includeArtist']\n",
    "    )\n",
    "\n",
    "    modelConfig['test_loss'] = test_loss\n",
    "    modelConfig['test_acc'] = test_acc\n",
    "\n",
    "    print(f\"Training complete. Pickling results\")\n",
    "    modelFileName = 'saves/' + modelConfig['name'] + '.pkl'\n",
    "    modelFile = open(modelFileName, 'ab')\n",
    "    pickle.dump(modelConfig, modelFile)                    \n",
    "    modelFile.close()\n",
    "\n",
    "    kerasFileName = 'saves/' + modelConfig['name'] + '.keras'\n",
    "    modelConfig['model'].save(kerasFileName)\n",
    "    print(f\"\\n\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f443ecc-8a03-485b-b434-34b0c27aa8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557bcb83-f3d5-4820-b408-a4509bfb6b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelFileName = 'saves/' + modelConfig['name'] + '.pkl'\n",
    "# modelFileName = 'smtest.pkl'\n",
    "# modelFileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f24fd-1c55-40a8-acb6-27b1eddfd2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelFileName = 'saves/allModelsRun1.pkl'\n",
    "modelFile = open(modelFileName, 'ab')\n",
    "pickle.dump(modelConfigs, modelFile)                    \n",
    "modelFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8a5d25-4f28-419f-80f3-62ef8031761a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9311a24-eb4e-4058-bb3e-d013061106be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
